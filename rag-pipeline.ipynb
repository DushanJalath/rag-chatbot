{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml,os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cadentials.yaml\") as f:\n",
    "    cadentials=yaml.load(f,Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(\n",
    "            openai_api_key=cadentials['OPENAI_API_KEY'],\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            temperature=0.5,\n",
    "            max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm_embeddings=HuggingFaceBgeEmbeddings(\n",
    "                                    model_name = 'BAAI/bge-small-en-v1.5',\n",
    "                                    model_kwargs = {'device' : 'cpu'},\n",
    "                                    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_dir):\n",
    "    pdf_docs = [os.path.join(pdf_dir, pdf) for pdf in os.listdir(pdf_dir)]\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text):\n",
    "    textt_splitter=CharacterTextSplitter(\n",
    "                                        separator=\"\\n\",\n",
    "                                        chunk_size=1000,\n",
    "                                        chunk_overlap=200\n",
    "    )\n",
    "    chunks=textt_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def vect_store(chunks):\n",
    "    #FAISS - FaceBook AI Similarity Search\n",
    "    vectStore=FAISS.from_texts(\n",
    "                            texts=chunks,\n",
    "                            embedding=llm_embeddings\n",
    "    )\n",
    "    return vectStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(pdf_docs):\n",
    "    text = get_pdf_text(pdf_docs)\n",
    "    chunks = chunk_text(text)\n",
    "    vectorstore = vect_store(chunks)\n",
    "    return vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vectorstore):\n",
    "    memory = ConversationBufferMemory(\n",
    "                                    memory_key='chat_history', \n",
    "                                    return_messages=True\n",
    "                                    )\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "                                                            llm=llm,\n",
    "                                                            retriever=vectorstore.as_retriever(),\n",
    "                                                            memory=memory\n",
    "                                                            )\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HYPOTHESIS TESTING\\nDr. Romesh Thanuja\\n(UOM) CM2420 1 / 9OUTLINE\\n1INTRODUCTION\\n2SIMPLE VERSUS COMPOSITE HYPOTHESIS\\n3TESTSTATISTICS\\n4DECISION RULE\\n5ERROR PROBABILITIES\\n6POWER FUNCTION\\n(UOM) CM2420 2 / 9INTRODUCTION\\nINTRODUCTION\\nDEFINITION\\nA hypothesis is a statement about a population parameter.\\nDEFINITION\\nThe two complementary hypotheses in a hypothesis testing problem are called the null hypothesis\\nand the alternative hypothesis. they are denoted by H0andH1.\\nGENERAL HYPOTHESIS\\nFor a vector of parameters θand parameter space Θ, we test\\nH0:θ∈Θ0vsH1:θ∈Θc\\n0\\nNote that Θ0andΘc\\n0must be mutually exclusive such that it is impossible for H0andH1to be true\\nsimultaneously.\\n(UOM) CM2420 3 / 9SIMPLE VERSUS COMPOSITE HYPOTHESIS\\nSIMPLE VERSUS COMPOSITE HYPOTHESIS\\nSIMPLE VERSUS COMPOSITE HYPOTHESIS\\nA simple hypothesis takes the form\\nH0:θ=c\\nwhere cis a constant, i.e. it states an exact specification of θ.\\nIf a vector of parameters is being tested, then we may write H0:θ=c, where cis a vector of',\n",
       " 'A simple hypothesis takes the form\\nH0:θ=c\\nwhere cis a constant, i.e. it states an exact specification of θ.\\nIf a vector of parameters is being tested, then we may write H0:θ=c, where cis a vector of\\nconstants of the same dimension as θ.\\nA hypothesis which is not simple is called a composite hypothesis, for example\\nH0:θ≤c.\\n(UOM) CM2420 4 / 9TESTSTATISTICS\\nTESTSTATISTICS\\nThe crucial part of a hypothesis test is to identify an appropriate test statistic, T(X).\\nOne of the desired features of T(X)is to have an interpretation such that large values of it\\nprovide evidence against H0.\\nWe also want to know the distribution of T(X)under H0, that is when H0holds, we require the\\ndistribution of T(X)|H0\\n(UOM) CM2420 5 / 9DECISION RULE\\nDECISION RULE\\nClassical hypothesis testing involves a binary choice such that we: reject H0or not reject H0\\nNote a hypothesis testing procedure is incapable of ”accepting” a hypothesis- a test can only',\n",
       " 'Classical hypothesis testing involves a binary choice such that we: reject H0or not reject H0\\nNote a hypothesis testing procedure is incapable of ”accepting” a hypothesis- a test can only\\ncheck to what extent the available evidence is consistent or inconsistent with the null\\nhypothesis.\\nThe evidence used to make the decision is given by the observed sample\\nx={x1,x2, . . . , xn}. A test statistics, T(X), is constructed and the computed test statistics\\nvalue, T(x), is used to make the decision of whether or not to reject H0.\\nThe decision-making process can be summarized using a single function δ(x), as follows:\\nδ(x) =(\\n1 when H0is rejected\\n0 when H0is not rejected\\nThe function δis known as the decision rule. δpartitions the space of possible observed\\nsamples into two sets:\\nR={x:δ(x) =1}andRc={x:δ(x) =0}\\nwhere the set Ris the rejection region (critical region) and Rcis the non-rejection region.\\n(UOM) CM2420 6 / 9ERROR PROBABILITIES\\nERROR PROBABILITIES',\n",
       " 'R={x:δ(x) =1}andRc={x:δ(x) =0}\\nwhere the set Ris the rejection region (critical region) and Rcis the non-rejection region.\\n(UOM) CM2420 6 / 9ERROR PROBABILITIES\\nERROR PROBABILITIES\\nA hypothesis test of H0:θ∈Θ0versus H1:θ∈Θc\\n0might make one of two types of errors.\\nIfθ∈Θ0, but the hypothesis test incorrectly decides to reject H0, then the test has made a\\nType I error.\\nIfθ∈Θ1but the hypothesis test decides to accept H0, a Type II error has been made.\\nSuppose Rdenotes the rejection region for a test. Then for θ∈Θ0, the test will make a\\nmistake if x∈R, so the probability of a Type I Error is Pθ(X∈R)\\nForθ∈Θc\\n0, the probability of a Type II Error is P(X∈Rc)(Note that\\nPθ(X∈Rc) =1−Pθ(X∈R)).\\nWe have\\nPθ(X∈R) =(\\nprobability of a Type I Error if θ∈Θ0\\none minus the probability of a Type II Error if θ∈Θc\\n0\\n(UOM) CM2420 7 / 9POWER FUNCTION\\nPOWER FUNCTION\\nDEFINITION\\nThe power function of a hypothesis test with rejection region Ris the function of θdefined by\\nβ(θ) =Pθ(X∈R)\\nDEFINITION',\n",
       " '0\\n(UOM) CM2420 7 / 9POWER FUNCTION\\nPOWER FUNCTION\\nDEFINITION\\nThe power function of a hypothesis test with rejection region Ris the function of θdefined by\\nβ(θ) =Pθ(X∈R)\\nDEFINITION\\nFor 0≤α≤1, a test with power function β(θ)is a size αtest if supθ∈Θoβ(θ) =α\\nDEFINITION\\nFor 0≤α≤1, a test with power function β(θ)is a level (significant) αtest if supθ∈Θoβ(θ)≤α\\n(UOM) CM2420 8 / 9POWER FUNCTION\\np-VALUE\\np-VALUE\\nLetT(X)be a test statistic used in a hypothesis testing procedure of a scalar parameter θ.\\nWithout loss of generality, suppose H0is rejected for large values of T(X). For an observed\\nsample xthe corresponding p-value is\\np(x) =supθ∈Θ0P(T(X)≥T(x))\\nwhere T(x)is the test statistics value.\\nNote that: If we have a fixed significance level α, then we can describe the rejection region as:\\nR={x:p(x)≤α}\\n(UOM) CM2420 9 / 9INTERVAL ESTIMATION\\nDr. Romesh Thanuja\\n(UOM) CM2420 1 / 6OUTLINE\\n1INTRODUCTION\\n2INTERVAL ESTIMATOR VS INTERVAL ESTIMATES\\n3FINDING INTERVAL ESTIMATORS FROM PIVOTAL FUNCTIONS',\n",
       " '(UOM) CM2420 9 / 9INTERVAL ESTIMATION\\nDr. Romesh Thanuja\\n(UOM) CM2420 1 / 6OUTLINE\\n1INTRODUCTION\\n2INTERVAL ESTIMATOR VS INTERVAL ESTIMATES\\n3FINDING INTERVAL ESTIMATORS FROM PIVOTAL FUNCTIONS\\n4CONSTRUCTING AN INTERVAL FROM A PIVOTAL FUNCTION\\n(UOM) CM2420 2 / 6INTRODUCTION\\nINTRODUCTION\\nIn the previous chapter, we discussed methods for obtaining point estimators of unknown\\nparameters.\\nIn this chapter, we focus on learning about interval estimators, which provide a range of\\npossible values for an unknown parameter rather than just a single point estimate.\\n(UOM) CM2420 3 / 6INTERVAL ESTIMATOR VS INTERVAL ESTIMATES\\nINTERVAL ESTIMATOR VS INTERVAL ESTIMATES\\nLetX={X1,X2, . . . , Xn}be a random sample from FXparameterised by θ. Let T1(X)andT2(X)\\nbe satisfies such that Ti(X)≤T2(X)for any observed sample x.\\nThe random interval:\\n[T1(X),T2(X)]\\nis an interval estimator of θ.\\nIf the observed sample is x, then the interval:\\n[T1(x),T2(x)]\\nis an interval estimate (or confidence interval) of θ.',\n",
       " 'The random interval:\\n[T1(X),T2(X)]\\nis an interval estimator of θ.\\nIf the observed sample is x, then the interval:\\n[T1(x),T2(x)]\\nis an interval estimate (or confidence interval) of θ.\\n(UOM) CM2420 4 / 6FINDING INTERVAL ESTIMATORS FROM PIVOTAL FUNCTIONS\\nFINDING INTERVAL ESTIMATORS FROM PIVOTAL FUNCTIONS\\nA way to construct 100 (1−α)%confidence set for θis by using a pivotal function.\\nPIVOTAL FUNCTION\\nConsider a random sample Xfrom FXparameterised by θ, where θis a scalar parameter.\\nSuppose that we are interested in constructing an interval estimator of θ.\\nLetg(X, θ)be a function of Xandθwhich does not involve any unknown parameters other than θ.\\ng(X, θ)is a pivotal function if its distribution is known and does not depend on θ.\\nNote that a pivotal function defines a random variable, say Y=g(X, θ). By definition, the\\ndistribution of Ydoes not depend on θ.\\n(UOM) CM2420 5 / 6CONSTRUCTING AN INTERVAL FROM A PIVOTAL FUNCTION\\nCONSTRUCTING AN INTERVAL FROM A PIVOTAL FUNCTION',\n",
       " 'distribution of Ydoes not depend on θ.\\n(UOM) CM2420 5 / 6CONSTRUCTING AN INTERVAL FROM A PIVOTAL FUNCTION\\nCONSTRUCTING AN INTERVAL FROM A PIVOTAL FUNCTION\\nSuppose that we have a random sample Xfrom FXparameterized by θ. To construct an interval\\nestimator of θwith a 100 (1−α)%confidence level using a pivotal function, we use the following\\nprocedure.\\n1Find a pivotal function g(X, θ)based on a reasonable point estimator.\\n2Use the distribution of the pivotal function to find values k1andk2such that:\\nP(k1≤g(X, θ)≤k2) =1−α\\n3Manipulate g(X, θ)≥k1andg(X, θ)≤k2to make θthe reference point. This yields\\ninequalities of the form:\\nθ≥T1(X,k1,k2)andθ≤T2(X,k1,k2)\\nfor some function T1(·)andT2(·)independent of θ.\\n4Construct the following interval\\n[T1(X,k1,k2),T2(X,k1,k2)]\\nwhich is then an interval estimator of θwith a confidence coefficient of 1 −α.\\nNote that endpoints T1andT2are usually functions of one (and only one) of k1andk2.\\n(UOM) CM2420 6 / 6POINT ESTIMATION\\nDr. Romesh Thanuja',\n",
       " 'Note that endpoints T1andT2are usually functions of one (and only one) of k1andk2.\\n(UOM) CM2420 6 / 6POINT ESTIMATION\\nDr. Romesh Thanuja\\n(UOM) CM2420 1 / 13OUTLINE\\n1INTRODUCTION\\n2METHOD OF FINDING ESTIMATORS\\nMaximum Likelihood Estimator\\nMethod of Moments\\n3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 2 / 13INTRODUCTION\\nPOINT ESTIMATION -INTRODUCTION\\nIn one common estimation scenario, we take a random sample from the distribution to elicit some\\ninformation about the unknown parameter θ. That is, we repeat the experiment nindependent\\ntimes, observe the sample, X1,X2, ...,Xn, and try to estimate the value of θby using the\\nobservations x1,x2, ...,xn. The function of X1,X2, ...,Xnused to estimate θ—say, the statistic\\nu(X1,X2, ...,Xn)—is called an estimator of θ. We want it to be such that the computed estimate\\nu(x1,x2, ...,xn)is usually close to θ. Since we are estimating one member of θ∈Ω, such an',\n",
       " 'u(X1,X2, ...,Xn)—is called an estimator of θ. We want it to be such that the computed estimate\\nu(x1,x2, ...,xn)is usually close to θ. Since we are estimating one member of θ∈Ω, such an\\nestimator is often called a point estimator.\\n(UOM) CM2420 3 / 13METHOD OF FINDING ESTIMATORS MAXIMUM LIKELIHOOD ESTIMATOR\\nOUTLINE\\n1INTRODUCTION\\n2METHOD OF FINDING ESTIMATORS\\nMaximum Likelihood Estimator\\nMethod of Moments\\n3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 4 / 13METHOD OF FINDING ESTIMATORS MAXIMUM LIKELIHOOD ESTIMATOR\\nMAXIMUM LIKELIHOOD ESTIMATION (MLE)\\nLetX1,X2, . . . , Xnbe a random sample from a distribution then their joint likelihood is denoted by\\nf(x, θ)orf(x1,x2, . . . , xn;θ). IfX1,X2, . . . , Xnare independent then the joint density function (pdf)\\nor pmf can be written as\\nf(x, θ) =f(x1;θ)f(x2;θ), . . . , f(xn;θ)\\nMaximum likelihood estimation is a method to estimate parameters and these parameter values',\n",
       " 'or pmf can be written as\\nf(x, θ) =f(x1;θ)f(x2;θ), . . . , f(xn;θ)\\nMaximum likelihood estimation is a method to estimate parameters and these parameter values\\nare fixed such that they maximize the likelihood function that the process produced the data that\\nwas actually observed.\\nThe joint pdf above when considered as a function of the unknown parameter θinstead of the\\nsample x, is known as the likelihood function.\\nL(θ) =f(x, θ) =f(x1;θ)f(x2;θ), . . . , f(xn;θ)\\nUsing the methods of calculus, we now find the values of θthat maximize L(θ).\\n(UOM) CM2420 5 / 13METHOD OF FINDING ESTIMATORS METHOD OF MOMENTS\\nOUTLINE\\n1INTRODUCTION\\n2METHOD OF FINDING ESTIMATORS\\nMaximum Likelihood Estimator\\nMethod of Moments\\n3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 6 / 13METHOD OF FINDING ESTIMATORS METHOD OF MOMENTS\\nMETHOD OF MOMENTS (MOM)',\n",
       " '3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 6 / 13METHOD OF FINDING ESTIMATORS METHOD OF MOMENTS\\nMETHOD OF MOMENTS (MOM)\\nLetX1,X2, . . . , Xnbe a sample from a population with pdf or pmf f(x|θ1, θ2, . . . , θ k). Method of\\nmoments estimators are found by equating the first ksample moments to the corresponding k\\npopulation moments, and solving the resulting system of simultaneous equations. More precisely,\\ndefine\\nm1=1\\nnX\\nX1\\ni, µ′\\n1=E(X1),\\nm2=1\\nnX\\nX2\\ni, µ′\\n2=E(X2),\\n...\\nmk=1\\nnX\\nXk\\ni, µ′\\nk=E(Xk).\\nThe population moment µ′\\njwill typically be a function of θ1, θ2, . . . , θ ksayµ′\\nj(θ1, . . . , θ k).The\\nmethod of moments estimator (˜θ1, . . . , ˜θk)of(θ1, . . . , θ k)is obtained by solving the following\\nsystem of equations for (θ1, . . . , θ k)in terms of (m1, . . . , mk) :\\nm1=µ′\\n1(θ1, . . . , θ k),\\n...\\nmk=µ′\\nk(θ1, . . . , θ k).\\n(UOM) CM2420 7 / 13METHODS OF EVALUATING ESTIMATORS MEANSQUARED ERROR\\nOUTLINE',\n",
       " 'm1=µ′\\n1(θ1, . . . , θ k),\\n...\\nmk=µ′\\nk(θ1, . . . , θ k).\\n(UOM) CM2420 7 / 13METHODS OF EVALUATING ESTIMATORS MEANSQUARED ERROR\\nOUTLINE\\n1INTRODUCTION\\n2METHOD OF FINDING ESTIMATORS\\nMaximum Likelihood Estimator\\nMethod of Moments\\n3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 8 / 13METHODS OF EVALUATING ESTIMATORS MEANSQUARED ERROR\\nMETHODS OF EVALUATING ESTIMATORS\\nIn this section, we will consider some basic criteria for evaluating estimators.\\nMEANSQUARED ERROR\\nThe mean squared error (MSE) of an estimator Wof a parameter θis the function of θdefined by\\nEθ(W−θ)2.\\nMSE can be expressed as follows:\\nEθ(W−θ)2=VarθW+ (EθW−θ)2=VarθW+ (BiasθW)2.\\nBIAS OF AN ESTIMATOR\\nThe bias of a point estimator Wof a parameter θis the difference between the expected value of\\nWandθ; that is, Bias θW=EθW−θ. An estimator whose bias is identically equal to 0 is called\\nunbiased and satisfies EθW=θfor all θ.',\n",
       " 'Wandθ; that is, Bias θW=EθW−θ. An estimator whose bias is identically equal to 0 is called\\nunbiased and satisfies EθW=θfor all θ.\\n(UOM) CM2420 9 / 13METHODS OF EVALUATING ESTIMATORS BESTUNBIASED ESTIMATORS\\nOUTLINE\\n1INTRODUCTION\\n2METHOD OF FINDING ESTIMATORS\\nMaximum Likelihood Estimator\\nMethod of Moments\\n3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 10 / 13METHODS OF EVALUATING ESTIMATORS BESTUNBIASED ESTIMATORS\\nBESTUNBIASED ESTIMATORS\\nUNIFORM MINIMUM VARIANCE UNBIASED ESTIMATOR (UMVUE)\\nAn estimator W∗is a best unbiased estimator of τ(θ)if it satisfies EθW∗=τ(θ)for all θand, for\\nany other estimator WwithEθW=τ(θ), we have VarθW∗≤VarθWfor all θ.W∗is also called a\\nuniform minimum variance unbiased estimator (UMVUE) of τ(θ).\\nCRAMER -RAOINEQUALITY\\nLetX1,X2, . . . , Xnbe a sample with pdf f(x|θ)and let W(X) =W(X1, . . . , Xn)be any estimator\\nsatisfying\\nd\\ndθEθW(X) =Z\\nχ∂\\n∂θ[W(x)f(x|θ)]dx\\nand\\nVarθW(X)<∞.\\nThen\\nVarθ(W(X))≥(d',\n",
       " 'CRAMER -RAOINEQUALITY\\nLetX1,X2, . . . , Xnbe a sample with pdf f(x|θ)and let W(X) =W(X1, . . . , Xn)be any estimator\\nsatisfying\\nd\\ndθEθW(X) =Z\\nχ∂\\n∂θ[W(x)f(x|θ)]dx\\nand\\nVarθW(X)<∞.\\nThen\\nVarθ(W(X))≥(d\\ndθEθW(X))2\\nEθ\\x10\\n(∂\\n∂θlogf(X|θ))2\\x11\\n(UOM) CM2420 11 / 13METHODS OF EVALUATING ESTIMATORS SUFFICIENCY AND UNBIASEDNESS\\nOUTLINE\\n1INTRODUCTION\\n2METHOD OF FINDING ESTIMATORS\\nMaximum Likelihood Estimator\\nMethod of Moments\\n3METHODS OF EVALUATING ESTIMATORS\\nMean Squared Error\\nBest Unbiased Estimators\\nSufficiency and Unbiasedness\\n(UOM) CM2420 12 / 13METHODS OF EVALUATING ESTIMATORS SUFFICIENCY AND UNBIASEDNESS\\nSUFFICIENCY AND UNBIASEDSNES\\nRECALL\\nIfXandYare any two random variables, then provided the expectations exist, we have\\nE(X) =E[E(X|Y)],\\nVar(X) =Var[E(X|Y)] +E[Var(X|Y)]\\nRAO-BLACKWELL THEOREM\\nLetWbe any unbiased estimator of τ(θ), and let Tbe a sufficient statistics for θ. Define\\nϕ(T) =E(W|T). Then Eθϕ(T) =τ(θ)andVarθϕ(T)≤VarθWfor all θ; that is, ϕ(T)is a\\nuniformly better unbiased estimator of τ(θ)',\n",
       " 'ϕ(T) =E(W|T). Then Eθϕ(T) =τ(θ)andVarθϕ(T)≤VarθWfor all θ; that is, ϕ(T)is a\\nuniformly better unbiased estimator of τ(θ)\\n(UOM) CM2420 13 / 13']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=get_pdf_text('./data/')\n",
    "chunks=chunk_text(texts)\n",
    "vectorStore=vect_store(chunks)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x1cf006f6f30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory(conversation_chain):\n",
    "    histroy = \"\"\n",
    "    memory_elements = conversation_chain.memory.chat_memory\n",
    "    for idx, element in enumerate(memory_elements.messages):\n",
    "        message = element.content\n",
    "        if idx % 2 == 0:\n",
    "            histroy += f\"user: {message}\\n\"\n",
    "        else:\n",
    "            histroy += f\"bot: {message}\\n\\n\"\n",
    "    print(histroy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferMemory(return_messages=True, memory_key='chat_history'), combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001CF5958EC30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001CF597E2F60>, temperature=0.5, openai_api_key='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', openai_proxy='', max_tokens=500)), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001CF5958EC30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001CF597E2F60>, temperature=0.5, openai_api_key='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', openai_proxy='', max_tokens=500)), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x000001CF006F6F30>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain = get_conversation_chain(vectorStore)\n",
    "conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know the current president of Sri Lanka.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"Who is president of SriLanka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don\\'t have information on \"flash attention.\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"Wht is flash attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a statistical model. When you have a random sample of data, the likelihood function is constructed based on the joint probability distribution of the sample. The likelihood function is essentially a measure of how likely it is that the observed data came from a specific distribution with certain parameters.\\n\\nThe goal of MLE is to find the values of the parameters that maximize this likelihood function. In other words, it seeks to find the parameter values that make the observed data the most probable. This is done by taking the derivative of the likelihood function with respect to the parameters and setting it to zero to find the maximum.\\n\\nOnce the maximum likelihood estimates are obtained, they are considered the best estimates for the parameters given the data. MLE is widely used in statistics and is a powerful tool for parameter estimation in various fields like economics, biology, and engineering.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"simply teach me maximum likelihood estimation \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The likelihood function, denoted as L(θ), is the joint probability distribution of the sample data considered as a function of the unknown parameter θ. It is used in statistics to find the values of the parameter θ that maximize the likelihood of the observed data being produced by the process. In other words, the likelihood function helps in estimating the unknown parameter by determining the values that make the observed data most probable. This is done through methods like Maximum Likelihood Estimation, where the parameter values are chosen to maximize the likelihood function.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"simply teach me likelihood function \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of the likelihood function, the joint probability refers to the probability of observing a set of data points or samples given a specific parameter value. When considering the joint probability as a function of the unknown parameter θ instead of the sample x, it is known as the likelihood function.\\n\\nFor example, if we have a random sample X1, X2, ..., Xn from a distribution with a parameter θ, the joint likelihood is denoted by f(x, θ) or f(x1, x2, ..., xn; θ). If the samples X1, X2, ..., Xn are independent, then the joint density function can be written as f(x, θ) = f(x1; θ) * f(x2; θ) * ... * f(xn; θ).\\n\\nThe likelihood function is used in maximum likelihood estimation to find the parameter values that maximize the likelihood that the observed data was produced by the process. By maximizing the likelihood function with respect to the parameter θ, we can estimate the most likely value of the parameter given the observed data.\\n\\nIn summary, the joint probability in the context of the likelihood function represents the probability of observing a set of data points or samples given a specific parameter value, and it is used to estimate the unknown parameter through maximum likelihood estimation.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"what is known as the joint probability in above case? explain me more about the join probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have the specific information on joint probability in the context provided.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"explain me joint probability with a simple real world example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.run(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.run(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.run(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.run(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.run(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
